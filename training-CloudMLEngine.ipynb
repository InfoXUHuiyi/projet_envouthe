{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'huiyi-sandbox'\n",
    "PROJECT = 'huiyi-training'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://huiyi-sandbox/preproc/eval.csv-00000-of-00001\n",
      "gs://huiyi-sandbox/preproc/train.csv-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on Cloud ML Engine requires:\n",
    "\n",
    "1. Making the code a Python package\n",
    "2. Using gcloud to submit the training code to Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tuto/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%writefile tuto/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--bucket',\n",
    "        help = 'GCS path to data. We assume that data is in gs://BUCKET/preproc/',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help = 'GCS location to write checkpoints and export models',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        help = 'Number of examples to compute gradient over.',\n",
    "        type = int,\n",
    "        default = 128\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help = 'this model ignores this field, but it is required by gcloud',\n",
    "        default = 'junk'\n",
    "    )\n",
    " \n",
    "    ## TODO 1: add the new arguments here \n",
    "    parser.add_argument(\n",
    "        '--train_examples',\n",
    "        help = 'Number of examples (in thousands) to run the training job over. If this is more than actual # of examples available,\\\n",
    "        it cycles through them. So specifying 1000 here when you have only 100k examples makes this 10 epochs.',\n",
    "        type = int,\n",
    "        default = 5000\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        '--pattern',\n",
    "        help = 'Specify a pattern that has to be in input files. For example 00001-of will process only one shard',\n",
    "        default = 'of'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_steps',\n",
    "        help = 'Positive number of steps for which to evaluate model. Default to None, \\\n",
    "        which means to evaluate until input_fn raises an end-of-input exception',\n",
    "        type = int,       \n",
    "        default = None\n",
    "    )\n",
    "        \n",
    "    ## parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # unused args provided by service\n",
    "    #arguments.pop('job_dir', None)\n",
    "    arguments.pop('job-dir', None)\n",
    "\n",
    "    ## assign the arguments to the model variables\n",
    "    output_dir = arguments.pop('output_dir')\n",
    "    model.BUCKET     = arguments.pop('bucket')\n",
    "    model.BATCH_SIZE = arguments.pop('batch_size')\n",
    "    model.TRAIN_STEPS = (arguments.pop('train_examples') * 1000) / model.BATCH_SIZE\n",
    "    model.EVAL_STEPS = arguments.pop('eval_steps')    \n",
    "    print (\"Will train for {} steps using batch_size={}\".format(model.TRAIN_STEPS, model.BATCH_SIZE))\n",
    "    model.PATTERN = arguments.pop('pattern')\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get('TF_CONFIG', '{}')\n",
    "        ).get('task', {}).get('trial', '')\n",
    "    )\n",
    "    \n",
    "    print(\"outpur_dir: {}\".format(output_dir))\n",
    "    \n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tuto/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile tuto/trainer/model.py\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "BUCKET = None  # set from task.py\n",
    "PATTERN = 'of' # gets all files\n",
    "\n",
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = 'SALE_PRICE,BOROUGH,BLOCK,ZIP_CODE,key'.split(',')\n",
    "LABEL_COLUMN = 'SALE_PRICE'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[600000.0], ['1'], [1418.0], [10065.0],['nokey']]\n",
    "\n",
    "# Define some hyperparameters\n",
    "TRAIN_STEPS = 10000\n",
    "EVAL_STEPS = None\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(prefix, mode, batch_size=128):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "        \n",
    "        # Use prefix to create file path\n",
    "        file_path = 'gs://{}/preproc/{}*{}*'.format(BUCKET, prefix, PATTERN)\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(file_path)\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = (tf.data.TextLineDataset(file_list)  # Read text file\n",
    "                    .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "              \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size) #melanger les donnees\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    " \n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "  \n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "#Create feature columns for estimator\n",
    "def make_feature_cols():\n",
    "    # Define column types\n",
    "    INPUT_COLUMNS = \\\n",
    "        [\\\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list('BOROUGH', ['1', '2', '3', '4', '5']),\n",
    "            tf.feature_column.numeric_column('BLOCK'),\n",
    "            tf.feature_column.numeric_column('ZIP_CODE')\n",
    "        ]\n",
    "    \n",
    "    return INPUT_COLUMNS\n",
    "\n",
    "\n",
    "# Create serving input function to be able to serve predictions later using provided inputs\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        'BOROUGH': tf.placeholder(tf.string, [None]),\n",
    "        'BLOCK': tf.placeholder(tf.string, [None]),\n",
    "        'ZIP_CODE': tf.placeholder(tf.string, [None]),\n",
    "        KEY_COLUMN: tf.placeholder_with_default(tf.constant(['nokey']), [None])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "# create metric for hyperparameter tuning\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = predictions['predictions']\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "  \n",
    "#create estimator to train and evaluate\n",
    "def train_and_evaluate(OUTDIR):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    #Linear Regression with tf.Estimator framework\n",
    "    shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    #create model of linear regression\n",
    "    #estimator = tf.estimator.LinearRegressor(feature_columns = make_feature_cols())\n",
    "    estimator = tf.estimator.LinearRegressor(\n",
    "                       model_dir = OUTDIR,\n",
    "                       feature_columns = make_feature_cols())\n",
    "    \n",
    "    EVAL_INTERVAL = 300 # seconds\n",
    "\n",
    "    ## set the save_checkpoints_secs to the EVAL_INTERVAL\n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
    "                                        keep_checkpoint_max = 3)\n",
    "\n",
    "    # illustrates how to add an extra metric\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, my_rmse)\n",
    "    # for batch prediction, you need a key associated with each instance\n",
    "    estimator = tf.contrib.estimator.forward_features(estimator, KEY_COLUMN)\n",
    "\n",
    "    #Set the third argument of read_dataset to BATCH_SIZE and set max_steps to TRAIN_STEPS\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset('train', tf.estimator.ModeKeys.TRAIN, BATCH_SIZE),\n",
    "        max_steps = TRAIN_STEPS)\n",
    "    \n",
    "    #exporter = tf.estimator.LatestExporter('exporter', serving_input_fn, exports_to_keep=None)\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "\n",
    "    # Lastly, set steps equal to EVAL_STEPS\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset('eval', tf.estimator.ModeKeys.EVAL, 2**15),  # no need to batch in eval\n",
    "        steps = EVAL_STEPS,\n",
    "        start_delay_secs = 60, # start evaluating after N seconds\n",
    "        throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n",
    "        exporters = exporter)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure it works standalone. (Note the --pattern and --train_examples lines so that I am not trying to boil the ocean on my laptop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test en locale avant de deployer dans Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=huiyi-sandbox\n",
      "Will train for 7 steps using batch_size=128\n",
      "outpur_dir: tuto_trained/\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff196788d90>, '_model_dir': 'tuto_trained/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff1a95fbbd0>, '_model_dir': 'tuto_trained/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff1a95fbc90>, '_model_dir': 'tuto_trained/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /content/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /content/.local/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /content/.local/lib/python2.7/site-packages/tensorflow/python/ops/lookup_ops.py:1137: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-04-25 12:03:11.362918: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-04-25 12:03:11.368311: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2019-04-25 12:03:11.368524: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5603ad123ad0 executing computations on platform Host. Devices:\n",
      "2019-04-25 12:03:11.368542: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into tuto_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 674678000000000.0, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 7 into tuto_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-25T12:03:13Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /content/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from tuto_trained/model.ckpt-7\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-25-12:03:14\n",
      "INFO:tensorflow:Saving dict for global step 7: average_loss = 68661440000000.0, global_step = 7, label/mean = 1393804.8, loss = 1.1743166e+18, prediction/mean = 13256.64, rmse = 8286220.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7: tuto_trained/model.ckpt-7\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /content/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'BOROUGH': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'key': <tf.Tensor 'PlaceholderWithDefault:0' shape=(?,) dtype=string>, 'BLOCK': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'ZIP_CODE': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'BOROUGH': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'key': <tf.Tensor 'PlaceholderWithDefault:0' shape=(?,) dtype=string>, 'BLOCK': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, 'ZIP_CODE': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from tuto_trained/model.ckpt-7\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: tuto_trained/export/exporter/temp-1556193794/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 802604800000000.0.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf tuto_trained\n",
    "\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/tuto\n",
    "\n",
    "python -m trainer.task \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=tuto_trained \\\n",
    "  --job-dir=./train_tmp \\\n",
    "  --pattern=\"00000-of-\" --train_examples=1 --eval_steps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code works in standalone mode, you can run it on Cloud ML Engine. \n",
    "\n",
    "Because this is on the entire dataset, it will take a while.  You can monitor the job from the GCP console in the Cloud Machine Learning Engine section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://huiyi-sandbox/tuto/trained_model us-central1 tuto_190425_120443\n",
      "jobId: tuto_190425_120443\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://huiyi-sandbox/tuto/trained_model/#1556191188781780...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/checkpoint#1556191193028981...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/eval/#1556186912153037...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/eval/events.out.tfevents.1556186912.cmle-training-master-1b52619fce-0-crmnn#1556191198642390...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/events.out.tfevents.1556186886.cmle-training-master-1b52619fce-0-crmnn#1556191195564196...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/#1556186914807070...\n",
      "/ [1/78 objects]   1% Done                                                      \r",
      "/ [2/78 objects]   2% Done                                                      \r",
      "/ [3/78 objects]   3% Done                                                      \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/#1556186915097740...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556186913/#1556186921431443...\n",
      "/ [4/78 objects]   5% Done                                                      \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556186913/saved_model.pb#1556186921853423...\n",
      "/ [5/78 objects]   6% Done                                                      \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556186913/variables/#1556186922226030...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556186913/variables/variables.data-00000-of-00001#1556186922593389...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556186913/variables/variables.index#1556186923027212...\n",
      "/ [6/78 objects]   7% Done                                                      \r",
      "/ [7/78 objects]   8% Done                                                      \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556187509/#1556187516339939...\n",
      "/ [8/78 objects]  10% Done                                                      \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556187509/saved_model.pb#1556187516690097...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556187509/variables/#1556187517117569...\n",
      "/ [9/78 objects]  11% Done                                                      \r",
      "/ [10/78 objects]  12% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556187509/variables/variables.data-00000-of-00001#1556187517570366...\n",
      "/ [11/78 objects]  14% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556187509/variables/variables.index#1556187517900852...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188111/#1556188118326458...\n",
      "/ [12/78 objects]  15% Done                                                     \r",
      "/ [13/78 objects]  16% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188111/saved_model.pb#1556188118776600...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188111/variables/#1556188119187741...\n",
      "/ [14/78 objects]  17% Done                                                     \r",
      "/ [15/78 objects]  19% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188111/variables/variables.data-00000-of-00001#1556188119525752...\n",
      "/ [16/78 objects]  20% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188111/variables/variables.index#1556188119948104...\n",
      "/ [17/78 objects]  21% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188709/#1556188716437198...\n",
      "/ [18/78 objects]  23% Done                                                     \r",
      "/ [19/78 objects]  24% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188709/saved_model.pb#1556188716827865...\n",
      "/ [20/78 objects]  25% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188709/variables/#1556188717186503...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188709/variables/variables.data-00000-of-00001#1556188717616554...\n",
      "/ [21/78 objects]  26% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556188709/variables/variables.index#1556188718043672...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189310/#1556189316952784...\n",
      "/ [22/78 objects]  28% Done                                                     \r",
      "/ [23/78 objects]  29% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189310/saved_model.pb#1556189317389491...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189310/variables/#1556189317819894...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189310/variables/variables.data-00000-of-00001#1556189318256420...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189310/variables/variables.index#1556189318570509...\n",
      "/ [24/78 objects]  30% Done                                                     \r",
      "/ [25/78 objects]  32% Done                                                     \r",
      "/ [26/78 objects]  33% Done                                                     \r",
      "/ [27/78 objects]  34% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189911/#1556189918281395...\n",
      "/ [28/78 objects]  35% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189911/saved_model.pb#1556189918640700...\n",
      "/ [29/78 objects]  37% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189911/variables/#1556189918957458...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189911/variables/variables.data-00000-of-00001#1556189919302124...\n",
      "/ [30/78 objects]  38% Done                                                     \r",
      "/ [31/78 objects]  39% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556189911/variables/variables.index#1556189919662535...\n",
      "/ [32/78 objects]  41% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556190511/#1556190518265235...\n",
      "/ [33/78 objects]  42% Done                                                     \r",
      "/ [34/78 objects]  43% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556190511/saved_model.pb#1556190518715314...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556190511/variables/#1556190519057846...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556190511/variables/variables.data-00000-of-00001#1556190519473242...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556190511/variables/variables.index#1556190519887395...\n",
      "/ [35/78 objects]  44% Done                                                     \r",
      "/ [36/78 objects]  46% Done                                                     \r",
      "/ [37/78 objects]  47% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191111/#1556191117758435...\n",
      "/ [38/78 objects]  48% Done                                                     \r",
      "/ [39/78 objects]  50% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191111/saved_model.pb#1556191118099545...\n",
      "/ [40/78 objects]  51% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191111/variables/#1556191118457561...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191111/variables/variables.data-00000-of-00001#1556191118903681...\n",
      "/ [41/78 objects]  52% Done                                                     \r",
      "/ [42/78 objects]  53% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191111/variables/variables.index#1556191119328720...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191198/#1556191205740026...\n",
      "/ [43/78 objects]  55% Done                                                     \r",
      "/ [44/78 objects]  56% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191198/saved_model.pb#1556191206064519...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191198/variables/#1556191206425132...\n",
      "/ [45/78 objects]  57% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191198/variables/variables.data-00000-of-00001#1556191206737259...\n",
      "/ [46/78 objects]  58% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/export/exporter/1556191198/variables/variables.index#1556191207157212...\n",
      "/ [47/78 objects]  60% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/graph.pbtxt#1556186897650854...\n",
      "/ [48/78 objects]  61% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1089409.data-00000-of-00003#1556189903373758...\n",
      "/ [49/78 objects]  62% Done                                                     \r",
      "/ [50/78 objects]  64% Done                                                     \r",
      "/ [51/78 objects]  65% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1089409.data-00001-of-00003#1556189902812682...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1089409.data-00002-of-00003#1556189902270026...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1089409.index#1556189903842272...\n",
      "/ [52/78 objects]  66% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1089409.meta#1556189907796571...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1311258.data-00000-of-00003#1556190503317387...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1311258.data-00001-of-00003#1556190502651174...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1311258.data-00002-of-00003#1556190502188530...\n",
      "/ [53/78 objects]  67% Done                                                     \r",
      "/ [54/78 objects]  69% Done                                                     \r",
      "/ [55/78 objects]  70% Done                                                     \r",
      "/ [56/78 objects]  71% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1311258.index#1556190503930999...\n",
      "/ [57/78 objects]  73% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1311258.meta#1556190507992585...\n",
      "/ [58/78 objects]  74% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1531958.data-00000-of-00003#1556191103207469...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1531958.data-00001-of-00003#1556191102633096...\n",
      "/ [59/78 objects]  75% Done                                                     \r",
      "/ [60/78 objects]  76% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1531958.data-00002-of-00003#1556191102061678...\n",
      "/ [61/78 objects]  78% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1531958.index#1556191103607950...\n",
      "/ [62/78 objects]  79% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1531958.meta#1556191107659764...\n",
      "/ [63/78 objects]  80% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1562507.data-00000-of-00003#1556191190750600...\n",
      "/ [64/78 objects]  82% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1562507.data-00001-of-00003#1556191190282709...\n",
      "/ [65/78 objects]  83% Done                                                     \r",
      "/ [66/78 objects]  84% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1562507.data-00002-of-00003#1556191189754900...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1562507.index#1556191191348123...\n",
      "/ [67/78 objects]  85% Done                                                     \r",
      "/ [68/78 objects]  87% Done                                                     \r",
      "/ [69/78 objects]  88% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-1562507.meta#1556191195080120...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-868067.data-00000-of-00003#1556189303407838...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-868067.data-00001-of-00003#1556189302937931...\n",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-868067.data-00002-of-00003#1556189302360361...\n",
      "/ [70/78 objects]  89% Done                                                     \r",
      "/ [71/78 objects]  91% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-868067.index#1556189303907948...\n",
      "/ [72/78 objects]  92% Done                                                     \r",
      "/ [73/78 objects]  93% Done                                                     \r",
      "Removing gs://huiyi-sandbox/tuto/trained_model/model.ckpt-868067.meta#1556189306567459...\n",
      "/ [74/78 objects]  94% Done                                                     \r",
      "/ [75/78 objects]  96% Done                                                     \r",
      "/ [76/78 objects]  97% Done                                                     \r",
      "/ [77/78 objects]  98% Done                                                     \r",
      "/ [78/78 objects] 100% Done                                                     \r\n",
      "Operation completed over 78 objects.                                             \n",
      "Job [tuto_190425_120443] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe tuto_190425_120443\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs tuto_190425_120443\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/tuto/trained_model\n",
    "JOBNAME=tuto_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/tuto/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --train_examples=200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 12885. Click <a href=\"/_proxy/47975/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "12885"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/trained_model'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop the tensorboard after if you finish the monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped TensorBoard with pid 12885\n"
     ]
    }
   ],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "    TensorBoard().stop(pid)\n",
    "    print('Stopped TensorBoard with pid {}'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning\n",
    "\n",
    "All of these are command-line parameters to my program. To do hyperparameter tuning, create hyperparam.xml and pass it as --configFile. This step will take up to 2 hours -- you can increase maxParallelTrials or reduce maxTrials to get it done faster. Since maxParallelTrials is the number of initial seeds to start searching from, you don't want it to be too large; otherwise, all you have is a random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD_1\n",
    "  hyperparameters:\n",
    "    hyperparameterMetricTag: rmse\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 20\n",
    "    maxParallelTrials: 5\n",
    "    enableTrialEarlyStopping: True\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 218\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://huiyi-sandbox/tuto/hyperparam us-central1 tuto_190425_133139\n",
      "jobId: tuto_190425_133139\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [tuto_190425_133139] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe tuto_190425_133139\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs tuto_190425_133139\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/tuto/hyperparam\n",
    "JOBNAME=tuto_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/tuto/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --eval_steps=10 \\\n",
    "  --train_examples=20000\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat training\n",
    "\n",
    "This time with tuned parameters (note last line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://huiyi-sandbox/tuto/trained_model_tuned us-central1 tuto_190425_142441\n",
      "jobId: tuto_190425_142441\n",
      "state: QUEUED\n",
      "gs://huiyi-sandbox/tuto/trained_model_tuned us-central1 tuto_190425_142441\n",
      "jobId: tuto_190425_142441\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [tuto_190425_142441] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe tuto_190425_142441\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs tuto_190425_142441\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [tuto_190425_142441] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe tuto_190425_142441\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs tuto_190425_142441\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/tuto/trained_model_tuned\n",
    "JOBNAME=tuto_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/tuto/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --train_examples=20000 --batch_size=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
