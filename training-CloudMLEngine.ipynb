{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'huiyi-sandbox'\n",
    "PROJECT = 'huiyi-training'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://huiyi-sandbox/preproc/eval.csv-00000-of-00001\n",
      "gs://huiyi-sandbox/preproc/train.csv-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on Cloud ML Engine requires:\n",
    "\n",
    "1. Making the code a Python package\n",
    "2. Using gcloud to submit the training code to Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tuto/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%writefile tuto/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--bucket',\n",
    "        help = 'GCS path to data. We assume that data is in gs://BUCKET/preproc/',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help = 'GCS location to write checkpoints and export models',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        help = 'Number of examples to compute gradient over.',\n",
    "        type = int,\n",
    "        default = 128\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help = 'this model ignores this field, but it is required by gcloud',\n",
    "        default = 'junk'\n",
    "    )\n",
    " \n",
    "    ## TODO 1: add the new arguments here \n",
    "    parser.add_argument(\n",
    "        '--train_examples',\n",
    "        help = 'Number of examples (in thousands) to run the training job over. If this is more than actual # of examples available,\\\n",
    "        it cycles through them. So specifying 1000 here when you have only 100k examples makes this 10 epochs.',\n",
    "        type = int,\n",
    "        default = 5000\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        '--pattern',\n",
    "        help = 'Specify a pattern that has to be in input files. For example 00001-of will process only one shard',\n",
    "        default = 'of'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_steps',\n",
    "        help = 'Positive number of steps for which to evaluate model. Default to None, \\\n",
    "        which means to evaluate until input_fn raises an end-of-input exception',\n",
    "        type = int,       \n",
    "        default = None\n",
    "    )\n",
    "        \n",
    "    ## parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # unused args provided by service\n",
    "    arguments.pop('job_dir', None)\n",
    "    arguments.pop('job-dir', None)\n",
    "\n",
    "    ## assign the arguments to the model variables\n",
    "    output_dir = arguments.pop('output_dir')\n",
    "    model.BUCKET     = arguments.pop('bucket')\n",
    "    model.BATCH_SIZE = arguments.pop('batch_size')\n",
    "    model.TRAIN_STEPS = (arguments.pop('train_examples') * 1000) / model.BATCH_SIZE\n",
    "    model.EVAL_STEPS = arguments.pop('eval_steps')    \n",
    "    print (\"Will train for {} steps using batch_size={}\".format(model.TRAIN_STEPS, model.BATCH_SIZE))\n",
    "    model.PATTERN = arguments.pop('pattern')\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get('TF_CONFIG', '{}')\n",
    "        ).get('task', {}).get('trial', '')\n",
    "    )\n",
    "\n",
    "    print(\"outpur_dir: {}\".format(output_dir))\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tuto/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile tuto/trainer/model.py\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "BUCKET = None  # set from task.py\n",
    "PATTERN = 'of' # gets all files\n",
    "\n",
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = 'SALE_PRICE,BOROUGH,BLOCK,ZIP_CODE,key'.split(',')\n",
    "LABEL_COLUMN = 'SALE_PRICE'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[600000.0], ['1'], [1418.0], [10065.0],['nokey']]\n",
    "\n",
    "# Define some hyperparameters\n",
    "TRAIN_STEPS = 10000\n",
    "EVAL_STEPS = None\n",
    "BATCH_SIZE = 128\n",
    "#NEMBEDS = 3\n",
    "#QUEUE_CAPACITY = 1000\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(prefix, mode, batch_size):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "        \n",
    "        # Use prefix to create file path\n",
    "        file_path = 'gs://{}/preproc/{}*{}*'.format(BUCKET, prefix, PATTERN)\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(file_path)\n",
    "        #print('file_list={}'.format(file_list))\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = (tf.data.TextLineDataset(file_list)  # Read text file\n",
    "                    .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "              \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size) #melanger les donnees\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    " \n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "  \n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "#Create feature columns for estimator\n",
    "def make_feature_cols():\n",
    "    # Define column types\n",
    "    BOROUGH,BLOCK,ZIP_CODE = \\\n",
    "        [\\\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list('BOROUGH', ['1', '2', '3', '4', '5']),\n",
    "            tf.feature_column.numeric_column('BLOCK'),\n",
    "            tf.feature_column.numeric_column('ZIP_CODE')\n",
    "        ]\n",
    "\n",
    "    input_columns = [BOROUGH,BLOCK,ZIP_CODE]\n",
    "    \n",
    "    return input_columns\n",
    "\n",
    "\n",
    "# Create serving input function to be able to serve predictions later using provided inputs\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        'BOROUGH': tf.placeholder(tf.string, [None]),\n",
    "        'BLOCK': tf.placeholder(tf.float32, [None]),\n",
    "        'ZIP_CODE': tf.placeholder(tf.float32, [None]),\n",
    "        KEY_COLUMN: tf.placeholder_with_default(tf.constant(['nokey']), [None])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "# create metric for hyperparameter tuning\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = predictions['predictions']\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "  \n",
    "#create estimator to train and evaluate\n",
    "def train_and_evaluate(OUTDIR):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    #Linear Regression with tf.Estimator framework\n",
    "    shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    #create model of linear regression\n",
    "    estimator = tf.estimator.LinearRegressor(feature_columns = make_feature_cols())\n",
    "    \n",
    "    EVAL_INTERVAL = 300 # seconds\n",
    "\n",
    "    ## set the save_checkpoints_secs to the EVAL_INTERVAL\n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
    "                                        keep_checkpoint_max = 3)\n",
    "\n",
    "    # illustrates how to add an extra metric\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, my_rmse)\n",
    "    # for batch prediction, you need a key associated with each instance\n",
    "    estimator = tf.contrib.estimator.forward_features(estimator, KEY_COLUMN)\n",
    "\n",
    "    #Set the third argument of read_dataset to BATCH_SIZE and set max_steps to TRAIN_STEPS\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset('train', tf.estimator.ModeKeys.TRAIN, BATCH_SIZE),\n",
    "        max_steps = TRAIN_STEPS)\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn, exports_to_keep=None)\n",
    "\n",
    "    # Lastly, set steps equal to EVAL_STEPS\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset('eval', tf.estimator.ModeKeys.EVAL, 2**15),  # no need to batch in eval\n",
    "        steps = EVAL_STEPS,\n",
    "        start_delay_secs = 60, # start evaluating after N seconds\n",
    "        throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n",
    "        exporters = exporter)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure it works standalone. (Note the --pattern and --train_examples lines so that I am not trying to boil the ocean on my laptop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test en locale avant de deployer dans Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=huiyi-sandbox\n",
      "Will train for 7 steps using batch_size=128\n",
      "outpur_dir: tuto_trained/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpwIKhmd\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f76d2bb0550>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpwIKhmd', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f76d2bb05d0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpwIKhmd', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f76cd978310>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpwIKhmd', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 300 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-04-24 09:50:41.820154: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpwIKhmd/model.ckpt.\n",
      "INFO:tensorflow:loss = 1465029500000000.0, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 7 into /tmp/tmpwIKhmd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 580485370000000.0.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-24-09:50:43\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpwIKhmd/model.ckpt-7\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-24-09:50:44\n",
      "INFO:tensorflow:Saving dict for global step 7: average_loss = 68662500000000.0, global_step = 7, loss = 1.1743348e+18, rmse = 8286284.0\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'BOROUGH': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'key': <tf.Tensor 'PlaceholderWithDefault:0' shape=(?,) dtype=string>, 'BLOCK': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'ZIP_CODE': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'BOROUGH': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'key': <tf.Tensor 'PlaceholderWithDefault:0' shape=(?,) dtype=string>, 'BLOCK': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'ZIP_CODE': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpwIKhmd/model.ckpt-7\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/tmpwIKhmd/export/exporter/temp-1556099444/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf tuto_trained\n",
    "\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/tuto\n",
    "\n",
    "python -m trainer.task \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=tuto_trained \\\n",
    "  --job-dir=./train_tmp \\\n",
    "  --pattern=\"00000-of-\" --train_examples=1 --eval_steps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code works in standalone mode, you can run it on Cloud ML Engine. \n",
    "\n",
    "Because this is on the entire dataset, it will take a while.  You can monitor the job from the GCP console in the Cloud Machine Learning Engine section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://huiyi-sandbox/tuto/trained_model us-central1 tuto_190424_095054\n",
      "jobId: tuto_190424_095054\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [tuto_190424_095054] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe tuto_190424_095054\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs tuto_190424_095054\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/tuto/trained_model\n",
    "JOBNAME=tuto_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/tuto/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --train_examples=200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/trained_model'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop the tensorboard after if you finish the monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print('Stopped TensorBoard with pid {}'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning\n",
    "\n",
    "All of these are command-line parameters to my program. To do hyperparameter tuning, create hyperparam.xml and pass it as --configFile. This step will take up to 2 hours -- you can increase maxParallelTrials or reduce maxTrials to get it done faster. Since maxParallelTrials is the number of initial seeds to start searching from, you don't want it to be too large; otherwise, all you have is a random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%writefile hyperparam.yaml\n",
    "#trainingInput:\n",
    "#  scaleTier: STANDARD_1\n",
    "#  hyperparameters:\n",
    "#    hyperparameterMetricTag: rmse\n",
    "#    goal: MINIMIZE\n",
    "#    maxTrials: 20\n",
    "#    maxParallelTrials: 5\n",
    "#    enableTrialEarlyStopping: True\n",
    "#    params:\n",
    "#    - parameterName: batch_size\n",
    "#      type: INTEGER\n",
    "#      minValue: 8\n",
    "#      maxValue: 128\n",
    "#      scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%bash\n",
    "#OUTDIR=gs://${BUCKET}/hyperparam\n",
    "#JOBNAME=tuto_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "#echo $OUTDIR $REGION $JOBNAME\n",
    "#gsutil -m rm -rf $OUTDIR\n",
    "\n",
    "#gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "#  --region=$REGION \\\n",
    "#  --module-name=trainer.task \\\n",
    "#  --package-path=$(pwd)/trainer \\\n",
    "#  --job-dir=$OUTDIR \\\n",
    "#  --staging-bucket=gs://$BUCKET \\\n",
    "#  --scale-tier=STANDARD_1 \\\n",
    "#  --config=hyperparam.yaml \\\n",
    "#  --runtime-version=$TFVERSION \\\n",
    "#  -- \\\n",
    "#  --bucket=${BUCKET} \\\n",
    "#  --output_dir=${OUTDIR} \\\n",
    "#  --eval_steps=10 \\\n",
    "#  --train_examples=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat training\n",
    "\n",
    "This time with tuned parameters (note last line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%bash\n",
    "#OUTDIR=gs://${BUCKET}/trained_model_tuned\n",
    "#JOBNAME=tuto_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "#echo $OUTDIR $REGION $JOBNAME\n",
    "#gsutil -m rm -rf $OUTDIR\n",
    "\n",
    "#gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "#  --region=$REGION \\\n",
    "#  --module-name=trainer.task \\\n",
    "#  --package-path=$(pwd)/trainer \\\n",
    "#  --job-dir=$OUTDIR \\\n",
    "#  --staging-bucket=gs://$BUCKET \\\n",
    "#  --scale-tier=STANDARD_1 \\\n",
    "#  --runtime-version=$TFVERSION \\\n",
    "#  -- \\\n",
    "#  --bucket=${BUCKET} \\\n",
    "#  --output_dir=${OUTDIR} \\\n",
    "#  --train_examples=20000 --batch_size=35"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
