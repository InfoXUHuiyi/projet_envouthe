{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing avec Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to run this notebook in the environment of python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda update -y -n base -c defaults conda\n",
    "source activate py2env\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "conda install -y pytz\n",
    "pip install apache-beam[gcp]==2.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a pip install, click **\"Reset Session\"** on the notebook so that the Python environment picks up the new packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'test_envouthe'\n",
    "PROJECT = 'envouthe-datalake'\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En fonction de la notebook *envouthe_base_algo_test*, on decide Random Rorest comme l'algorithme, et les features sont 'id_subscriptionMain','abonnement_consec', 'end_year' et 'total_sub'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_subscriptionMain</th>\n",
       "      <th>abonnement_consec</th>\n",
       "      <th>end_year</th>\n",
       "      <th>total_sub</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16422</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16422</td>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16422</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16422</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16396</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_subscriptionMain  abonnement_consec  end_year  total_sub churn\n",
       "0                16422                  1      2014          1    No\n",
       "1                16422                  2      2014          2    No\n",
       "2                16422                  3      2014          3    No\n",
       "3                16422                  4      2014          4    No\n",
       "4                16396                  1      2014          1    No"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call BigQuery and examine in dataframe\n",
    "import google.datalab.bigquery as bq\n",
    "\n",
    "query = \"\"\"\n",
    " SELECT\n",
    "  id_subscriptionMain,\n",
    "  abonnement_consec,\n",
    "  EXTRACT(year FROM ending_month) AS end_year,\n",
    "  total_sub,\n",
    "  churn___ as churn\n",
    " FROM\n",
    "  renew_abonnement.merged_ds\n",
    " LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "dataset = bq.Query(query).execute().result().to_dataframe()\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# echantillonner directement dataset aux train, valid et test set\n",
    "\n",
    "# shuffled = dataset.sample(frac=1)\n",
    "# trainsize = int(len(shuffled) * 0.70)\n",
    "# validsize = int(len(shuffled) * 0.15)\n",
    "# # df_local = shuffled.iloc[:100]\n",
    "# df_train = shuffled.iloc[:trainsize, :]\n",
    "# df_valid = shuffled.iloc[trainsize:(trainsize+validsize), :]\n",
    "# df_test = shuffled.iloc[(trainsize+validsize):, :]\n",
    "\n",
    "# print(df_train.shape)\n",
    "# print(df_valid.shape)\n",
    "# print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder train,valid,test dataset dans google storage\n",
    "\n",
    "# import google.datalab.storage as storage\n",
    "\n",
    "# df_train.to_csv('train.csv',encoding='utf-8') #sauvegarder dans le local\n",
    "# df_valid.to_csv('valid.csv',encoding='utf-8')\n",
    "# df_test.to_csv('test.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %bash\n",
    "# gsutil cp 'train.csv' \"gs://$BUCKET/data/\"\n",
    "# gsutil cp 'valid.csv' \"gs://$BUCKET/data/\"\n",
    "# gsutil cp 'test.csv' \"gs://$BUCKET/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on va lire les donnees par la requete BigQuery, puis on fait un peu de preprocessing ici: convertir les valeurs categoriques en numeriques, puis echantilloner les donnees en train set et eval set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import datetime, os\n",
    "\n",
    "def to_csv(rowdict):\n",
    "  # Pull columns from BQ and create a line\n",
    "    import hashlib\n",
    "    import copy\n",
    "    \n",
    "    CSV_COLUMNS = \"churn,abonnement_consec,end_year,total_sub\".split(',')\n",
    "\n",
    "    df = copy.deepcopy(rowdict)\n",
    "\n",
    "    for keys in [df]:\n",
    "      data = ','.join([str(keys[k]) if k in keys else 'None' for k in CSV_COLUMNS])  \n",
    "      key = hashlib.sha224(data).hexdigest()  # hash the columns to form a key\n",
    "      yield str('{},{}'.format(data, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format of variables:\n",
    "- rowdict = {u'churn': u'No', u'abonnement_consec': 1, u'end_year': 2014, u'total_sub': 1}\n",
    "- csv_columns = [u'churn', u'abonnement_consec', u'end_year', u'total_sub']\n",
    "- data = No,1 2014,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-envouthe-base-190823-132641 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:800: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  options = pbegin.pipeline.options.view_as(DebugOptions)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(in_test_mode):\n",
    "    import shutil, os, subprocess\n",
    "    \n",
    "    job_name = 'preprocess-envouthe-base' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "\n",
    "    if in_test_mode:\n",
    "        print('Launching local job ... hang on')\n",
    "        OUTPUT_DIR = './preproc'\n",
    "        shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "    else:\n",
    "        print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "        OUTPUT_DIR = 'gs://{0}/envouthe_base/preproc/'.format(BUCKET)\n",
    "        try:\n",
    "          subprocess.check_call('gsutil -m rm -r {}'.format(OUTPUT_DIR).split())\n",
    "        except:\n",
    "          pass\n",
    "\n",
    "    options = {\n",
    "      'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "      'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "      'job_name': job_name,\n",
    "      'project': PROJECT,\n",
    "      'region': REGION,\n",
    "      'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "      'no_save_main_session': True,\n",
    "      'max_num_workers': 6\n",
    "    }\n",
    "\n",
    "    opts = beam.pipeline.PipelineOptions(flags = [], **options)\n",
    "\n",
    "    if in_test_mode:\n",
    "        RUNNER = 'DirectRunner'\n",
    "    else:\n",
    "        RUNNER = 'DataflowRunner'\n",
    "\n",
    "    p = beam.Pipeline(RUNNER, options = opts)\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      int64_field_0 AS row_id,\n",
    "      id_subscriptionMain,\n",
    "      abonnement_consec,\n",
    "      EXTRACT(year FROM ending_month) AS end_year,\n",
    "      total_sub,\n",
    "      (CASE\n",
    "        WHEN churn___ = 'No' THEN 0\n",
    "        WHEN churn___ = 'Yes' THEN 1\n",
    "        WHEN churn___ = 'Renew' THEN 2\n",
    "        ELSE 3 END) AS churn\n",
    "     FROM\n",
    "      renew_abonnement.merged_ds\n",
    "    \"\"\"\n",
    "\n",
    "    train_query = 'SELECT * EXCEPT(row_id) FROM ({}) WHERE row_id < (SELECT count(*) FROM `renew_abonnement.telecom_churn` )*0.70'.format(query)\n",
    "\n",
    "    eval_query = 'SELECT * EXCEPT(row_id) FROM ({}) WHERE row_id >= (SELECT count(*) FROM `renew_abonnement.telecom_churn` )*0.70 \\\n",
    "    and row_id <= (SELECT count(*) FROM `renew_abonnement.telecom_churn` )'.format(query)\n",
    "\n",
    "    if in_test_mode:\n",
    "        train_query = train_query + ' LIMIT 100'\n",
    "        eval_query = eval_query + ' LIMIT 100'\n",
    "\n",
    "    for step in ['train', 'eval']:\n",
    "      if step == 'train':\n",
    "          selquery = train_query\n",
    "      else:\n",
    "          selquery = eval_query\n",
    "\n",
    "      (p \n",
    "       | '{}_read'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query = selquery, use_standard_sql = True))\n",
    "       | '{}_csv'.format(step) >> beam.FlatMap(to_csv)\n",
    "       | '{}_out'.format(step) >> beam.io.Write(beam.io.WriteToText(os.path.join(OUTPUT_DIR, '{}.csv'.format(step))))\n",
    "      )\n",
    "\n",
    "    job = p.run()\n",
    "    if in_test_mode:\n",
    "      job.wait_until_finish()\n",
    "      print(\"Done!\")\n",
    "    \n",
    "preprocess(in_test_mode = False)\n",
    "# preprocess(in_test_mode = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualiser les fichiers train set et eval set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://test_envouthe/envouthe_base/preproc/eval.csv-00000-of-00001\n",
      "gs://test_envouthe/envouthe_base/preproc/train.csv-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/envouthe_base/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
